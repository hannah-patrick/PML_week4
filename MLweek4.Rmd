---
title: "Week4Project"
author: "Hannah Patrick"
date: "10/15/2017"
output: 
  html_document:
    keep_md: true
---


#Introduction
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. This is the "classe" variable in the training set. 



#Data

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

The data used here is from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).


The training data for this project is available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data is available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. 


```{r, include=FALSE}
library(ggplot2)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)

```


#Data Exploration
## Data Preparation/Cleaning

Read in the data:
```{r}
train <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"


traindata <- read.csv(url(train), na.strings=c("NA","#DIV/0!",""))
testdata <- read.csv(url(test), na.strings=c("NA","#DIV/0!",""))


```


The training dataset will be used to train and evaluate the model it is composed of 160 columns and 19622 rows. The test dataset will be used as a final test for this project. 

Some of the datatypes are different between the final test data and the training data. To fix this we will convert all intergers to numeric. 
```{r}


conv.to.numeric.training <- names(which(sapply(traindata, is.integer)==TRUE))
conv.to.numeric.test <- names(which(sapply(testdata, is.integer)==TRUE))

traindata[,conv.to.numeric.training] <- apply(traindata[,conv.to.numeric.training], 2, function(x) as.numeric(x))
testdata[,conv.to.numeric.test] <- apply(testdata[,conv.to.numeric.test], 2, function(x) as.numeric(x))
```

Splitting the training data further into a training and test data set will allow us to compare the accuracy of models produced and evaluate which method appears to be better suited for predictions on the dataset. The split here was chosen to be 70% train 30% test.

```{r}
set.seed(1234)
inTrain <- createDataPartition(traindata$classe, p=0.7, list=FALSE)
trainset <- traindata[inTrain, ]
testset <- traindata[-inTrain, ]
```

##Exploration
Distribution of the seen classes:
```{r}
table(trainset$classe)
```

Distribution of the class by person: 
```{r}
ggplot(trainset, aes(x=classe)) + geom_bar(aes(fill=user_name))
```

Class A is the most commonly seen class. Adelmo is the user with the most recorded exercises. 

It is not necessary to use all 160 columns in the prediction. These should be reduced to those that seem relevant. 

Removing the first column representing the row number:
```{r}
trainset <- trainset[,-1]
testset <- testset[,-1]

```

Remove near zero variance variables: 
```{r}

nzv <- nearZeroVar(trainset)
trainset <- trainset[,-nzv]
testset <- testset[,-nzv]
```


Look at the number of NA values for each variable:
```{r}

sapply(trainset, function(x) sum(is.na(x)))

```

Remove variables that contain NA values: 
```{r}

colsToKeep <- sapply(trainset, function(x) sum(is.na(x)))

colsToKeep <- which(colsToKeep==0)

trainset <- trainset[, colsToKeep]
testset <- testset[,colsToKeep]

# removing timestamp makes no sense to use as a predictor
trainset$cvtd_timestamp <- NULL
testset$cvtd_timestamp <- NULL

```

This leaves us with 56 predictor variables: 
```{r}
names(trainset)

```



#Creating Models 
Several models were created and compared using the accuracy on our test data set.

## Decision Trees

Create the decision tree model:
```{r}
set.seed(1234)
DTfit <- rpart(classe ~ ., data=trainset, method="class")
```

```{r, fig.width=10}
#plot the decision tree
rpart.plot(DTfit)
```

Use this model to predict the class of our testdata:
```{r}
DTpred <- predict(DTfit, testset, type = "class")
DTcm <- confusionMatrix(DTpred, testset$classe)
DTcm
```


From the above metrics we can see that the model produced the correct prediction 82% of the time. It was most accurate with Class A and least accurate at predicting Class D.

##Random Forests

Create the random forest model:
```{r}
set.seed(1234)

RFfit <- randomForest(classe ~ ., data=trainset)
RFpred <- predict(RFfit, testset, type = "class")
RFcm <- confusionMatrix(RFpred, testset$classe)
RFcm
```

From the above metrics we can see that the model produced near perfect predictions for all classes. With an overall accuracy of 0.9986 this is considerably better than the decision tree model. 


##Boosted Trees

The final model to be tested here is a gradient boosted machine using cross validation. 
This takes longer to run than the other models:
```{r}
set.seed(1234)
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 1)

GBMfit <- train(classe ~ ., data=trainset, method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE)

GBMpred <- predict(GBMfit, testset)
GBMcm <- confusionMatrix(GBMpred, testset$classe)
GBMcm

```

From the above metrics we can see that the model produced has an overall accuracy of 0.9963 this is only marginally different to the random forest model. The plot below shows the accuracy produced with varying tree depths and boosting iterations. A slight improvement might be accomplished by increasing the tree depth or the number of boosting iterations, however considering the time it would take to run and the marginal improvement it is not considered viable. 

```{r}
ggplot(GBMfit)
```


#Conclusion 
Using a random forest model produced the best results in this instance, with an accuracy of 0.9986% on the test dataset. This was considerably better than the decision tree and marginally better than the Gradient boosted method. The expected out of sample error rate is (1-0.9986)*100 = 0.14%. 



#Results on test set
The results for the 20 test examples are as follows:


```{r}

Finalpred <- predict(RFfit, testdata, type = "class")
Finalpred

```
```
















